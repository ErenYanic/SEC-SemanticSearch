{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "650ce215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting SEC EDGAR identity: Michael Mccallum, mike.mccalum@indigo.com\n",
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce GTX 1650\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Cell 1 — Imports & Configuration\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "import chromadb\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "from edgar import *\n",
    "from doc2dict import html2dict, unnest_dict\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load environment variables from .env\n",
    "load_dotenv()\n",
    "os.environ[\"HF_TOKEN\"] = os.getenv(\"HUGGING_FACE_TOKEN\", \"\")\n",
    "\n",
    "# Set SEC EDGAR identity\n",
    "# SEC EDGAR requires an identity string (name + email).\n",
    "# Read from edgar-identity.txt (line 1: name, line 2: email) to avoid\n",
    "# committing credentials to version control.\n",
    "identity_name = os.getenv(\"EDGAR_IDENTITY_NAME\")\n",
    "identity_email = os.getenv(\"EDGAR_IDENTITY_EMAIL\")\n",
    "\n",
    "if identity_name and identity_email:\n",
    "    print(f\"Setting SEC EDGAR identity: {identity_name}, {identity_email}\")\n",
    "    set_identity(f\"{identity_name} {identity_email}\")\n",
    "else:\n",
    "    print(\"Warning: SEC EDGAR identity not set. Please provide name and email in environment variables.\")\n",
    "\n",
    "# ----- Constants -----\n",
    "EMBEDDING_MODEL_NAME = \"google/embeddinggemma-300m\"\n",
    "CHUNK_TOKEN_LIMIT = 500     # Hard upper bound (tokens ≈ whitespace-split words)\n",
    "CHUNK_TOLERANCE = 50        # Soft target: finalise a chunk once it reaches 450-550 tokens\n",
    "TOP_K = 5                   # Number of results to return\n",
    "\n",
    "# ----- Device -----\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "if DEVICE == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kw7lqm47wj",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filing: Filing(company='NVIDIA CORP', cik=1045810, form='10-K', filing_date='2025-02-26', accession_no='0001045810-25-000023')\n",
      "Filed: 2025-02-26\n",
      "\n",
      "HTML length: 2,067,520 characters\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Cell 2 — Fetch SEC Filing\n",
    "# =============================================================================\n",
    "\n",
    "TICKER = \"AAPL\"\n",
    "FORM_TYPE = \"10-K\"\n",
    "\n",
    "company = Company(TICKER)\n",
    "filings = company.get_filings(form=FORM_TYPE)\n",
    "\n",
    "# Retrieve the most recent filing\n",
    "filing = filings[0]\n",
    "print(f\"Filing: {filing}\")\n",
    "print(f\"Filed: {filing.filing_date}\")\n",
    "\n",
    "# Download the HTML content\n",
    "html_content = filing.html()\n",
    "print(f\"\\nHTML length: {len(html_content):,} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "t6zmykmi5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 562 raw segments\n",
      "\n",
      "[textsmall] OR > ☐☐TRANSITION REPORT PURSUANT TO SECTION 13 OR 15(d) OF THE SECURITIES EXCHANGE ACT OF 1934TRANSITION REPORT PURSUANT TO SECTION 13 OR 15(d) OF THE SECURITIES EXCHANGE ACT OF 1934TRANSITION REPORT PURSUANT TO SECTION 13 OR 15(d) OF THE SECURITIES EXCHANGE ACT OF 1934\n",
      "  Commission file number: 0-23985...\n",
      "\n",
      "[table] NVIDIA CORPORATION\n",
      "  (Exact name of registrant as specified in its charter) Delaware | 94-3177549 (State or other jurisdiction of | (I.R.S. E...\n",
      "\n",
      "[table] NVIDIA CORPORATION\n",
      "  Title of each class | Trading Symbol(s) | Name of each exchange on which registered Common Stock, $0.001 par value per s...\n",
      "\n",
      "[textsmall] NVIDIA CORPORATION\n",
      "  Large accelerated filer...\n",
      "\n",
      "[textsmall] NVIDIA CORPORATION\n",
      "  Large accelerated filerLarge accelerated filer☒☒☒Accelerated filerAccelerated filerAccelerated filer☐☐☐Non-accelerated f...\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Cell 3 — Parse HTML to Dictionary & Extract Segments\n",
    "# =============================================================================\n",
    "\n",
    "def extract_segments(dct, path=\"\", segments=None):\n",
    "    \"\"\"\n",
    "    Recursively traverse the doc2dict output and extract text segments\n",
    "    with their full hierarchical path (e.g. 'Part I > Item 1 > Business').\n",
    "\n",
    "    Each segment is a dict with keys: 'path', 'type', 'content'.\n",
    "    \"\"\"\n",
    "    if segments is None:\n",
    "        segments = []\n",
    "\n",
    "    if not isinstance(dct, dict):\n",
    "        return segments\n",
    "\n",
    "    # Build the current path from 'title' if present\n",
    "    current_path = path\n",
    "    if \"title\" in dct and isinstance(dct[\"title\"], str):\n",
    "        title = dct[\"title\"].strip()\n",
    "        if title:\n",
    "            current_path = f\"{path} > {title}\" if path else title\n",
    "\n",
    "    # Extract text content\n",
    "    for key in (\"text\", \"textsmall\"):\n",
    "        if key in dct and isinstance(dct[key], str):\n",
    "            text = dct[key].strip()\n",
    "            if text:\n",
    "                segments.append({\n",
    "                    \"path\": current_path or \"(root)\",\n",
    "                    \"type\": key,\n",
    "                    \"content\": text,\n",
    "                })\n",
    "\n",
    "    # Extract table content — convert to a readable string representation\n",
    "    if \"table\" in dct:\n",
    "        table = dct[\"table\"]\n",
    "        table_parts = []\n",
    "\n",
    "        if isinstance(table, dict):\n",
    "            if table.get(\"title\"):\n",
    "                table_parts.append(str(table[\"title\"]))\n",
    "            if table.get(\"preamble\"):\n",
    "                table_parts.append(str(table[\"preamble\"]))\n",
    "            if table.get(\"data\"):\n",
    "                for row in table[\"data\"]:\n",
    "                    table_parts.append(\" | \".join(str(cell) for cell in row))\n",
    "            if table.get(\"footnotes\"):\n",
    "                for fn in table[\"footnotes\"]:\n",
    "                    table_parts.append(str(fn))\n",
    "            if table.get(\"postamble\"):\n",
    "                table_parts.append(str(table[\"postamble\"]))\n",
    "        elif isinstance(table, list):\n",
    "            for row in table:\n",
    "                table_parts.append(\" | \".join(str(cell) for cell in row))\n",
    "\n",
    "        table_text = \"\\n\".join(table_parts).strip()\n",
    "        if table_text:\n",
    "            segments.append({\n",
    "                \"path\": current_path or \"(root)\",\n",
    "                \"type\": \"table\",\n",
    "                \"content\": table_text,\n",
    "            })\n",
    "\n",
    "    # Recurse into nested contents\n",
    "    contents = dct.get(\"contents\", {})\n",
    "    if isinstance(contents, dict):\n",
    "        for key in contents:\n",
    "            extract_segments(contents[key], current_path, segments)\n",
    "\n",
    "    return segments\n",
    "\n",
    "\n",
    "# Parse the filing HTML\n",
    "parsed = html2dict(html_content)\n",
    "\n",
    "# Handle the 'document' wrapper if present\n",
    "root = parsed.get(\"document\", parsed)\n",
    "all_segments = []\n",
    "if isinstance(root, dict):\n",
    "    for key in root:\n",
    "        extract_segments(root[key], segments=all_segments)\n",
    "\n",
    "print(f\"Extracted {len(all_segments):,} raw segments\")\n",
    "\n",
    "# Show a sample\n",
    "for seg in all_segments[:5]:\n",
    "    preview = seg[\"content\"][:120].replace(\"\\n\", \" \")\n",
    "    print(f\"\\n[{seg['type']}] {seg['path']}\")\n",
    "    print(f\"  {preview}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffbzpwdv8fh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks after splitting: 566\n",
      "Token range: 1 – 980\n",
      "Mean tokens per chunk: 92\n",
      "Chunks exceeding 500 tokens: 5\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Cell 4 — Chunk Long Segments (sentence-boundary aware)\n",
    "# =============================================================================\n",
    "\n",
    "def token_count(text):\n",
    "    \"\"\"Approximate token count using whitespace splitting.\"\"\"\n",
    "    return len(text.split())\n",
    "\n",
    "\n",
    "def chunk_segment(segment, limit=CHUNK_TOKEN_LIMIT, tolerance=CHUNK_TOLERANCE):\n",
    "    \"\"\"\n",
    "    Split a segment into chunks that respect the token limit.\n",
    "\n",
    "    Sentences are never cut in half. A chunk is finalised when the next\n",
    "    sentence would push it past the limit, even if the chunk ends at\n",
    "    e.g. 476 tokens (within the ±tolerance band).\n",
    "\n",
    "    Returns a list of segment dicts, each inheriting the original path.\n",
    "    \"\"\"\n",
    "    content = segment[\"content\"]\n",
    "    total_tokens = token_count(content)\n",
    "\n",
    "    # If the segment already fits, return it as-is\n",
    "    if total_tokens <= limit:\n",
    "        return [segment]\n",
    "\n",
    "    # Split on sentence boundaries (full stop, exclamation mark, question mark)\n",
    "    sentences = re.split(r\"(?<=[.!?])\\s+\", content)\n",
    "\n",
    "    chunks = []\n",
    "    current_sentences = []\n",
    "    current_tokens = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence_tokens = token_count(sentence)\n",
    "\n",
    "        # If a single sentence exceeds the limit, keep it whole rather than\n",
    "        # cutting mid-sentence — the tolerance band permits this.\n",
    "        if current_tokens + sentence_tokens > limit + tolerance and current_sentences:\n",
    "            chunks.append({\n",
    "                \"path\": segment[\"path\"],\n",
    "                \"type\": segment[\"type\"],\n",
    "                \"content\": \" \".join(current_sentences),\n",
    "            })\n",
    "            current_sentences = []\n",
    "            current_tokens = 0\n",
    "\n",
    "        current_sentences.append(sentence)\n",
    "        current_tokens += sentence_tokens\n",
    "\n",
    "    # Flush the remaining sentences\n",
    "    if current_sentences:\n",
    "        chunks.append({\n",
    "            \"path\": segment[\"path\"],\n",
    "            \"type\": segment[\"type\"],\n",
    "            \"content\": \" \".join(current_sentences),\n",
    "        })\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# Apply chunking to all segments\n",
    "chunks = []\n",
    "for seg in all_segments:\n",
    "    chunks.extend(chunk_segment(seg))\n",
    "\n",
    "print(f\"Total chunks after splitting: {len(chunks):,}\")\n",
    "\n",
    "# Show token distribution\n",
    "token_counts = [token_count(c[\"content\"]) for c in chunks]\n",
    "print(f\"Token range: {min(token_counts)} – {max(token_counts)}\")\n",
    "print(f\"Mean tokens per chunk: {sum(token_counts) / len(token_counts):.0f}\")\n",
    "\n",
    "# Show how many exceed the limit (should only be single long sentences)\n",
    "over_limit = sum(1 for t in token_counts if t > CHUNK_TOKEN_LIMIT)\n",
    "print(f\"Chunks exceeding {CHUNK_TOKEN_LIMIT} tokens: {over_limit}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "j8wspc4hntl",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86d0d1cdf60b447290014306e9072508",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/314 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 'google/embeddinggemma-300m' on cuda\n",
      "Embedding 566 chunks...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5aab0e70c0f24f3e84bed0804665ad16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 122.00 MiB. GPU 0 has a total capacity of 3.63 GiB of which 140.50 MiB is free. Including non-PyTorch memory, this process has 3.48 GiB memory in use. Of the allocated memory 2.82 GiB is allocated by PyTorch, and 607.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Generate embeddings (batch encoding on GPU)\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEmbedding \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(texts)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m chunks...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m embeddings = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_to_numpy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEmbedding shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00membeddings.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Initialise ChromaDB (persistent local storage)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Belgeler/SEC-SemanticSearch/uv_SEC_SemanticSearch/lib64/python3.13/site-packages/torch/utils/_contextlib.py:124\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    122\u001b[39m     \u001b[38;5;66;03m# pyrefly: ignore [bad-context-manager]\u001b[39;00m\n\u001b[32m    123\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Belgeler/SEC-SemanticSearch/uv_SEC_SemanticSearch/lib64/python3.13/site-packages/sentence_transformers/SentenceTransformer.py:1094\u001b[39m, in \u001b[36mSentenceTransformer.encode\u001b[39m\u001b[34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, truncate_dim, pool, chunk_size, **kwargs)\u001b[39m\n\u001b[32m   1091\u001b[39m features.update(extra_features)\n\u001b[32m   1093\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m-> \u001b[39m\u001b[32m1094\u001b[39m     out_features = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1095\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.device.type == \u001b[33m\"\u001b[39m\u001b[33mhpu\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1096\u001b[39m         out_features = copy.deepcopy(out_features)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Belgeler/SEC-SemanticSearch/uv_SEC_SemanticSearch/lib64/python3.13/site-packages/sentence_transformers/SentenceTransformer.py:1175\u001b[39m, in \u001b[36mSentenceTransformer.forward\u001b[39m\u001b[34m(self, input, **kwargs)\u001b[39m\n\u001b[32m   1169\u001b[39m             module_kwarg_keys = \u001b[38;5;28mself\u001b[39m.module_kwargs.get(module_name, [])\n\u001b[32m   1170\u001b[39m         module_kwargs = {\n\u001b[32m   1171\u001b[39m             key: value\n\u001b[32m   1172\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m kwargs.items()\n\u001b[32m   1173\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m module_kwarg_keys \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28mhasattr\u001b[39m(module, \u001b[33m\"\u001b[39m\u001b[33mforward_kwargs\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m module.forward_kwargs)\n\u001b[32m   1174\u001b[39m         }\n\u001b[32m-> \u001b[39m\u001b[32m1175\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1176\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Belgeler/SEC-SemanticSearch/uv_SEC_SemanticSearch/lib64/python3.13/site-packages/torch/nn/modules/module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Belgeler/SEC-SemanticSearch/uv_SEC_SemanticSearch/lib64/python3.13/site-packages/torch/nn/modules/module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Belgeler/SEC-SemanticSearch/uv_SEC_SemanticSearch/lib64/python3.13/site-packages/sentence_transformers/models/Transformer.py:262\u001b[39m, in \u001b[36mTransformer.forward\u001b[39m\u001b[34m(self, features, **kwargs)\u001b[39m\n\u001b[32m    239\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    240\u001b[39m \u001b[33;03mForward pass through the transformer model.\u001b[39;00m\n\u001b[32m    241\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    258\u001b[39m \u001b[33;03m        - 'all_layer_embeddings': If the model outputs hidden states, contains embeddings from all layers\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    260\u001b[39m trans_features = {key: value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m features.items() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model_forward_params}\n\u001b[32m--> \u001b[39m\u001b[32m262\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mauto_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtrans_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    263\u001b[39m token_embeddings = outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    264\u001b[39m features[\u001b[33m\"\u001b[39m\u001b[33mtoken_embeddings\u001b[39m\u001b[33m\"\u001b[39m] = token_embeddings\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Belgeler/SEC-SemanticSearch/uv_SEC_SemanticSearch/lib64/python3.13/site-packages/torch/nn/modules/module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Belgeler/SEC-SemanticSearch/uv_SEC_SemanticSearch/lib64/python3.13/site-packages/torch/nn/modules/module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Belgeler/SEC-SemanticSearch/uv_SEC_SemanticSearch/lib64/python3.13/site-packages/transformers/utils/generic.py:1001\u001b[39m, in \u001b[36mcheck_model_inputs.<locals>.wrapped_fn.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    999\u001b[39m             outputs = func(\u001b[38;5;28mself\u001b[39m, *args, **kwargs)\n\u001b[32m   1000\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1001\u001b[39m         outputs = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1002\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m original_exception:\n\u001b[32m   1003\u001b[39m     \u001b[38;5;66;03m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[39;00m\n\u001b[32m   1004\u001b[39m     \u001b[38;5;66;03m# Get a TypeError even after removing the recordable kwargs -> re-raise the original exception\u001b[39;00m\n\u001b[32m   1005\u001b[39m     \u001b[38;5;66;03m# Otherwise -> we're probably missing `**kwargs` in the decorated function\u001b[39;00m\n\u001b[32m   1006\u001b[39m     kwargs_without_recordable = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m recordable_keys}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Belgeler/SEC-SemanticSearch/uv_SEC_SemanticSearch/lib64/python3.13/site-packages/transformers/models/gemma3/modeling_gemma3.py:584\u001b[39m, in \u001b[36mGemma3TextModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, cache_position, **kwargs)\u001b[39m\n\u001b[32m    581\u001b[39m     position_embeddings[layer_type] = \u001b[38;5;28mself\u001b[39m.rotary_emb(hidden_states, position_ids, layer_type)\n\u001b[32m    583\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m decoder_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers[: \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers]:\n\u001b[32m--> \u001b[39m\u001b[32m584\u001b[39m     hidden_states = \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    585\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    586\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask_mapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdecoder_layer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mattention_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    587\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdecoder_layer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mattention_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    588\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    589\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    590\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    592\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    594\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.norm(hidden_states)\n\u001b[32m    596\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m BaseModelOutputWithPast(\n\u001b[32m    597\u001b[39m     last_hidden_state=hidden_states,\n\u001b[32m    598\u001b[39m     past_key_values=past_key_values,\n\u001b[32m    599\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Belgeler/SEC-SemanticSearch/uv_SEC_SemanticSearch/lib64/python3.13/site-packages/transformers/modeling_layers.py:93\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     90\u001b[39m         logger.warning_once(message)\n\u001b[32m     92\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Belgeler/SEC-SemanticSearch/uv_SEC_SemanticSearch/lib64/python3.13/site-packages/torch/nn/modules/module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Belgeler/SEC-SemanticSearch/uv_SEC_SemanticSearch/lib64/python3.13/site-packages/torch/nn/modules/module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Belgeler/SEC-SemanticSearch/uv_SEC_SemanticSearch/lib64/python3.13/site-packages/transformers/models/gemma3/modeling_gemma3.py:435\u001b[39m, in \u001b[36mGemma3DecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, position_embeddings, attention_mask, position_ids, past_key_values, cache_position, **kwargs)\u001b[39m\n\u001b[32m    433\u001b[39m residual = hidden_states\n\u001b[32m    434\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.pre_feedforward_layernorm(hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m435\u001b[39m hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    436\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.post_feedforward_layernorm(hidden_states)\n\u001b[32m    437\u001b[39m hidden_states = residual + hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Belgeler/SEC-SemanticSearch/uv_SEC_SemanticSearch/lib64/python3.13/site-packages/torch/nn/modules/module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Belgeler/SEC-SemanticSearch/uv_SEC_SemanticSearch/lib64/python3.13/site-packages/torch/nn/modules/module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Belgeler/SEC-SemanticSearch/uv_SEC_SemanticSearch/lib64/python3.13/site-packages/transformers/models/gemma3/modeling_gemma3.py:126\u001b[39m, in \u001b[36mGemma3MLP.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m     down_proj = \u001b[38;5;28mself\u001b[39m.down_proj(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mact_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgate_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mup_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    127\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 122.00 MiB. GPU 0 has a total capacity of 3.63 GiB of which 140.50 MiB is free. Including non-PyTorch memory, this process has 3.48 GiB memory in use. Of the allocated memory 2.82 GiB is allocated by PyTorch, and 607.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Cell 5 — Embed & Store in ChromaDB\n",
    "# =============================================================================\n",
    "\n",
    "# Load the embedding model on GPU\n",
    "model = SentenceTransformer(EMBEDDING_MODEL_NAME, device=DEVICE)\n",
    "print(f\"Loaded '{EMBEDDING_MODEL_NAME}' on {DEVICE}\")\n",
    "\n",
    "# Prepare texts and metadata\n",
    "texts = [c[\"content\"] for c in chunks]\n",
    "metadatas = [\n",
    "    {\n",
    "        \"path\": c[\"path\"],\n",
    "        \"type\": c[\"type\"],\n",
    "        \"ticker\": TICKER,\n",
    "        \"form_type\": FORM_TYPE,\n",
    "    }\n",
    "    for c in chunks\n",
    "]\n",
    "ids = [f\"{TICKER}_{FORM_TYPE}_{i}\" for i in range(len(chunks))]\n",
    "\n",
    "# Generate embeddings (batch encoding on GPU)\n",
    "print(f\"Embedding {len(texts):,} chunks...\")\n",
    "embeddings = model.encode(texts, batch_size=8, show_progress_bar=True, convert_to_numpy=True)\n",
    "print(f\"Embedding shape: {embeddings.shape}\")\n",
    "\n",
    "# Initialise ChromaDB (persistent local storage)\n",
    "chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "collection_name = f\"{TICKER}_{FORM_TYPE}\".lower().replace(\"-\", \"_\")\n",
    "\n",
    "# Delete existing collection if present (for clean re-runs)\n",
    "try:\n",
    "    chroma_client.delete_collection(name=collection_name)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "collection = chroma_client.create_collection(\n",
    "    name=collection_name,\n",
    "    metadata={\"hnsw:space\": \"cosine\"},\n",
    ")\n",
    "\n",
    "# Upsert into ChromaDB\n",
    "collection.add(\n",
    "    ids=ids,\n",
    "    embeddings=embeddings.tolist(),\n",
    "    documents=texts,\n",
    "    metadatas=metadatas,\n",
    ")\n",
    "print(f\"\\nStored {collection.count():,} chunks in collection '{collection_name}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fuo82bjm7fm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Cell 6 — Semantic Search Function\n",
    "# =============================================================================\n",
    "\n",
    "def semantic_search(query, top_k=TOP_K):\n",
    "    \"\"\"Embed the query and return the top-k most relevant chunks.\"\"\"\n",
    "    query_embedding = model.encode([query], convert_to_numpy=True).tolist()\n",
    "\n",
    "    results = collection.query(\n",
    "        query_embeddings=query_embedding,\n",
    "        n_results=top_k,\n",
    "        include=[\"documents\", \"metadatas\", \"distances\"],\n",
    "    )\n",
    "\n",
    "    print(f\"Query: \\\"{query}\\\"\\n\")\n",
    "    print(f\"{'Rank':<5} {'Score':<8} {'Section Path'}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    for i in range(len(results[\"ids\"][0])):\n",
    "        # ChromaDB returns cosine distance; similarity = 1 - distance\n",
    "        distance = results[\"distances\"][0][i]\n",
    "        similarity = 1 - distance\n",
    "        path = results[\"metadatas\"][0][i][\"path\"]\n",
    "        doc = results[\"documents\"][0][i]\n",
    "        seg_type = results[\"metadatas\"][0][i][\"type\"]\n",
    "\n",
    "        print(f\"\\n#{i + 1:<4} {similarity:.4f}  [{seg_type}] {path}\")\n",
    "        print(f\"     {doc[:200]}...\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "umovo9i4km",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: \"What are the main risk factors?\"\n",
      "\n",
      "Rank  Score    Section Path\n",
      "================================================================================\n",
      "\n",
      "#1    0.5201  [text] NVIDIA CORPORATION > We may not be able to realize the potential benefits of business investments or acquisitions, and we may not be able to successfully integrate acquired companies, which could hurt our ability to grow our business, develop new products or sell our products.\n",
      "     Additional risks related to acquisitions include, but are not limited to:...\n",
      "\n",
      "#2    0.4660  [text] NVIDIA CORPORATION > We may not be able to realize the potential benefits of business investments or acquisitions, and we may not be able to successfully integrate acquired companies, which could hurt our ability to grow our business, develop new products or sell our products.\n",
      "     •exposure to additional cybersecurity risks and vulnerabilities; and...\n",
      "\n",
      "#3    0.4459  [text] NVIDIA CORPORATION > Risk management and strategy\n",
      "     Refer to “Item 1A. Risk factors” in this annual report on Form 10-K for additional information about cybersecurity-related risks....\n",
      "\n",
      "#4    0.4173  [text] NVIDIA CORPORATION > Risks Related to Demand, Supply, and Manufacturing > Risks Related to Our Global Operating Business\n",
      "     •International sales and operations are a significant part of our business, which exposes us to risks that could harm our business....\n",
      "\n",
      "#5    0.4038  [text] NVIDIA CORPORATION > Risks Related to Demand, Supply, and Manufacturing > Risks Related to Our Global Operating Business\n",
      "     •Adverse economic conditions may harm our business....\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Query: \"Revenue and net income figures\"\n",
      "\n",
      "Rank  Score    Section Path\n",
      "================================================================================\n",
      "\n",
      "#1    0.4882  [table] NVIDIA CORPORATION > *$100 invested on 1/26/2020 in stock and in indices, including reinvestment of dividends. > Fiscal Year 2025 Summary > Results of Operations\n",
      "     Results of Operations\n",
      "A discussion regarding our financial condition and results of operations for fiscal year 2025 compared to fiscal year 2024 is presented below. A discussion regarding our financia...\n",
      "\n",
      "#2    0.4759  [table] NVIDIA CORPORATION > *$100 invested on 1/26/2020 in stock and in indices, including reinvestment of dividends. > Other Income (Expense), Net\n",
      "     | Year Ended\n",
      "Jan 26, 2025 | Year Ended\n",
      "Jan 28, 2024 | Year Ended\n",
      "$\n",
      "Change\n",
      " | ($ in millions) | ($ in millions) | ($ in millions)\n",
      "Interest income | $1,786 | $866 | $920\n",
      "Interest expense | (247) | (257)...\n",
      "\n",
      "#3    0.4691  [table] NVIDIA CORPORATION > (In millions)\n",
      "     (In millions)\n",
      " | Year Ended\n",
      "Jan 26, 2025 | Year Ended\n",
      "Jan 28, 2024 | Year Ended\n",
      "Jan 29, 2023\n",
      "Cash flows from operating activities: |  |  | \n",
      "Net income | $72,880 | $29,760 | $4,368\n",
      "Adjustments to recon...\n",
      "\n",
      "#4    0.4684  [table] NVIDIA CORPORATION > (In millions)\n",
      "     (In millions)\n",
      " | Year Ended\n",
      "Jan 26, 2025 | Year Ended\n",
      "Jan 28, 2024 | Year Ended\n",
      "Jan 29, 2023\n",
      "Net income | $72,880 | $29,760 | $4,368\n",
      "Other comprehensive income (loss), net of tax |  |  | \n",
      "Available-fo...\n",
      "\n",
      "#5    0.4646  [table] NVIDIA CORPORATION > *$100 invested on 1/26/2020 in stock and in indices, including reinvestment of dividends. > Fiscal Year 2025 Summary > Revenue by Reportable Segments\n",
      "     Revenue by Reportable Segments\n",
      " | Year Ended\n",
      "Jan 26, 2025 | Year Ended\n",
      "Jan 28, 2024 | Year Ended\n",
      "$\n",
      "Change | Year Ended\n",
      "%\n",
      "Change | Year Ended\n",
      "%\n",
      "Change\n",
      " | ($ in millions) | ($ in millions) | ($ in milli...\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Query: \"Supply chain and manufacturing operations\"\n",
      "\n",
      "Rank  Score    Section Path\n",
      "================================================================================\n",
      "\n",
      "#1    0.5854  [text] NVIDIA CORPORATION > Dependency on third-party suppliers and their technology to manufacture, assemble, test, or package our products reduces our control over product quantity and quality, manufacturing yields, and product delivery schedules and could harm our business.\n",
      "     •integration of new suppliers and contract manufacturers creating more complexity in managing multiple suppliers with variations in production planning, execution, and logistics;...\n",
      "\n",
      "#2    0.5565  [text] NVIDIA CORPORATION > Dependency on third-party suppliers and their technology to manufacture, assemble, test, or package our products reduces our control over product quantity and quality, manufacturing yields, and product delivery schedules and could harm our business.\n",
      "     •suppliers extending lead times and/or increasing costs during shortages; and...\n",
      "\n",
      "#3    0.5423  [text] NVIDIA CORPORATION > Risks Related to Demand, Supply, and Manufacturing\n",
      "     •Long manufacturing lead times and uncertain supply and component availability, combined with a failure to estimate customer demand accurately has led and could lead to mismatches between supply and d...\n",
      "\n",
      "#4    0.5242  [text] NVIDIA CORPORATION > Risks Related to Demand, Supply, and Manufacturing\n",
      "     •Dependency on third-party suppliers and their technology to manufacture, assemble, test, or package our products reduces our control over product quantity and quality, manufacturing yields, and produ...\n",
      "\n",
      "#5    0.5187  [text] NVIDIA CORPORATION > Competition > Patents and Proprietary Rights\n",
      "     •the location in which our products are manufactured;...\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Cell 7 — Demo Queries\n",
    "# =============================================================================\n",
    "\n",
    "# Example queries — adjust to match the filing content\n",
    "_ = semantic_search(\"What are the main risk factors?\")\n",
    "print(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
    "_ = semantic_search(\"Revenue and net income figures\")\n",
    "print(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
    "_ = semantic_search(\"Supply chain and manufacturing operations\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uv_SEC_SemanticSearch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

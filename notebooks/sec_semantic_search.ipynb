{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "650ce215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting SEC EDGAR identity: Michael Mccallum, mike.mccalum@indigo.com\n",
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce GTX 1650\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Cell 1 — Imports & Configuration\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "import chromadb\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "from edgar import *\n",
    "from doc2dict import html2dict, unnest_dict\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load environment variables from .env\n",
    "load_dotenv()\n",
    "os.environ[\"HF_TOKEN\"] = os.getenv(\"HUGGING_FACE_TOKEN\", \"\")\n",
    "\n",
    "# Set SEC EDGAR identity\n",
    "# SEC EDGAR requires an identity string (name + email).\n",
    "# Read from edgar-identity.txt (line 1: name, line 2: email) to avoid\n",
    "# committing credentials to version control.\n",
    "identity_name = os.getenv(\"EDGAR_IDENTITY_NAME\")\n",
    "identity_email = os.getenv(\"EDGAR_IDENTITY_EMAIL\")\n",
    "\n",
    "if identity_name and identity_email:\n",
    "    print(f\"Setting SEC EDGAR identity: {identity_name}, {identity_email}\")\n",
    "    set_identity(f\"{identity_name} {identity_email}\")\n",
    "else:\n",
    "    print(\"Warning: SEC EDGAR identity not set. Please provide name and email in environment variables.\")\n",
    "\n",
    "# ----- Constants -----\n",
    "EMBEDDING_MODEL_NAME = \"all-MiniLM-L6-v2\"\n",
    "CHUNK_TOKEN_LIMIT = 500     # Hard upper bound (tokens ≈ whitespace-split words)\n",
    "CHUNK_TOLERANCE = 50        # Soft target: finalise a chunk once it reaches 450-550 tokens\n",
    "TOP_K = 5                   # Number of results to return\n",
    "\n",
    "# ----- Device -----\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "if DEVICE == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kw7lqm47wj",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filing: Filing(company='NVIDIA CORP', cik=1045810, form='10-Q', filing_date='2023-11-21', accession_no='0001045810-23-000227')\n",
      "Filed: 2023-11-21\n",
      "\n",
      "HTML length: 1,470,759 characters\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Cell 2 — Fetch SEC Filing\n",
    "# =============================================================================\n",
    "\n",
    "TICKER = \"NVDA\"\n",
    "FORM_TYPE = \"10-Q\"\n",
    "\n",
    "company = Company(TICKER)\n",
    "filings = company.get_filings(form=FORM_TYPE)\n",
    "\n",
    "# Retrieve the most recent filing\n",
    "filing = filings[0]\n",
    "print(f\"Filing: {filing}\")\n",
    "print(f\"Filed: {filing.filing_date}\")\n",
    "\n",
    "# Download the HTML content\n",
    "html_content = filing.html()\n",
    "print(f\"\\nHTML length: {len(html_content):,} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "t6zmykmi5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 231 raw segments\n",
      "\n",
      "[table] introduction\n",
      "  introduction UNITED STATES SECURITIES AND EXCHANGE COMMISSION Washington, D.C. 20549...\n",
      "\n",
      "[textsmall] FORM 10-Q > ☒☒QUARTERLY REPORT PURSUANT TO SECTION 13 OR 15(d) OF THE SECURITIES EXCHANGE ACT OF 1934QUARTERLY REPORT PURSUANT TO SECTION 13 OR 15(d) OF THE SECURITIES EXCHANGE ACT OF 1934QUARTERLY REPORT PURSUANT TO SECTION 13 OR 15(d) OF THE SECURITIES EXCHANGE ACT OF 1934\n",
      "  For the quarterly period ended October 29, 2023...\n",
      "\n",
      "[textsmall] FORM 10-Q > ☒☒QUARTERLY REPORT PURSUANT TO SECTION 13 OR 15(d) OF THE SECURITIES EXCHANGE ACT OF 1934QUARTERLY REPORT PURSUANT TO SECTION 13 OR 15(d) OF THE SECURITIES EXCHANGE ACT OF 1934QUARTERLY REPORT PURSUANT TO SECTION 13 OR 15(d) OF THE SECURITIES EXCHANGE ACT OF 1934\n",
      "  OR...\n",
      "\n",
      "[textsmall] FORM 10-Q > ☐☐TRANSITION REPORT PURSUANT TO SECTION 13 OR 15(d) OF THE SECURITIES EXCHANGE ACT OF 1934TRANSITION REPORT PURSUANT TO SECTION 13 OR 15(d) OF THE SECURITIES EXCHANGE ACT OF 1934TRANSITION REPORT PURSUANT TO SECTION 13 OR 15(d) OF THE SECURITIES EXCHANGE ACT OF 1934\n",
      "  Commission file number: 0-23985...\n",
      "\n",
      "[table] NVIDIA CORPORATION\n",
      "  (Exact name of registrant as specified in its charter)  Delaware | 94-3177549 (State or other jurisdiction of | (I.R.S. ...\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Cell 3 — Parse HTML to Dictionary & Extract Segments\n",
    "# =============================================================================\n",
    "\n",
    "def extract_segments(dct, path=\"\", segments=None):\n",
    "    \"\"\"\n",
    "    Recursively traverse the doc2dict output and extract text segments\n",
    "    with their full hierarchical path (e.g. 'Part I > Item 1 > Business').\n",
    "\n",
    "    Each segment is a dict with keys: 'path', 'type', 'content'.\n",
    "    \"\"\"\n",
    "    if segments is None:\n",
    "        segments = []\n",
    "\n",
    "    if not isinstance(dct, dict):\n",
    "        return segments\n",
    "\n",
    "    # Build the current path from 'title' if present\n",
    "    current_path = path\n",
    "    if \"title\" in dct and isinstance(dct[\"title\"], str):\n",
    "        title = dct[\"title\"].strip()\n",
    "        if title:\n",
    "            current_path = f\"{path} > {title}\" if path else title\n",
    "\n",
    "    # Extract text content\n",
    "    for key in (\"text\", \"textsmall\"):\n",
    "        if key in dct and isinstance(dct[key], str):\n",
    "            text = dct[key].strip()\n",
    "            if text:\n",
    "                segments.append({\n",
    "                    \"path\": current_path or \"(root)\",\n",
    "                    \"type\": key,\n",
    "                    \"content\": text,\n",
    "                })\n",
    "\n",
    "    # Extract table content — convert to a readable string representation\n",
    "    if \"table\" in dct:\n",
    "        table = dct[\"table\"]\n",
    "        table_parts = []\n",
    "\n",
    "        if isinstance(table, dict):\n",
    "            if table.get(\"title\"):\n",
    "                table_parts.append(str(table[\"title\"]))\n",
    "            if table.get(\"preamble\"):\n",
    "                table_parts.append(str(table[\"preamble\"]))\n",
    "            if table.get(\"data\"):\n",
    "                for row in table[\"data\"]:\n",
    "                    table_parts.append(\" | \".join(str(cell) for cell in row))\n",
    "            if table.get(\"footnotes\"):\n",
    "                for fn in table[\"footnotes\"]:\n",
    "                    table_parts.append(str(fn))\n",
    "            if table.get(\"postamble\"):\n",
    "                table_parts.append(str(table[\"postamble\"]))\n",
    "        elif isinstance(table, list):\n",
    "            for row in table:\n",
    "                table_parts.append(\" | \".join(str(cell) for cell in row))\n",
    "\n",
    "        table_text = \"\\n\".join(table_parts).strip()\n",
    "        if table_text:\n",
    "            segments.append({\n",
    "                \"path\": current_path or \"(root)\",\n",
    "                \"type\": \"table\",\n",
    "                \"content\": table_text,\n",
    "            })\n",
    "\n",
    "    # Recurse into nested contents\n",
    "    contents = dct.get(\"contents\", {})\n",
    "    if isinstance(contents, dict):\n",
    "        for key in contents:\n",
    "            extract_segments(contents[key], current_path, segments)\n",
    "\n",
    "    return segments\n",
    "\n",
    "\n",
    "# Parse the filing HTML\n",
    "parsed = html2dict(html_content)\n",
    "\n",
    "# Handle the 'document' wrapper if present\n",
    "root = parsed.get(\"document\", parsed)\n",
    "all_segments = []\n",
    "if isinstance(root, dict):\n",
    "    for key in root:\n",
    "        extract_segments(root[key], segments=all_segments)\n",
    "\n",
    "print(f\"Extracted {len(all_segments):,} raw segments\")\n",
    "\n",
    "# Show a sample\n",
    "for seg in all_segments[:5]:\n",
    "    preview = seg[\"content\"][:120].replace(\"\\n\", \" \")\n",
    "    print(f\"\\n[{seg['type']}] {seg['path']}\")\n",
    "    print(f\"  {preview}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ffbzpwdv8fh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks after splitting: 232\n",
      "Token range: 1 – 550\n",
      "Mean tokens per chunk: 107\n",
      "Chunks exceeding 500 tokens: 1\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Cell 4 — Chunk Long Segments (sentence-boundary aware)\n",
    "# =============================================================================\n",
    "\n",
    "def token_count(text):\n",
    "    \"\"\"Approximate token count using whitespace splitting.\"\"\"\n",
    "    return len(text.split())\n",
    "\n",
    "\n",
    "def chunk_segment(segment, limit=CHUNK_TOKEN_LIMIT, tolerance=CHUNK_TOLERANCE):\n",
    "    \"\"\"\n",
    "    Split a segment into chunks that respect the token limit.\n",
    "\n",
    "    Sentences are never cut in half. A chunk is finalised when the next\n",
    "    sentence would push it past the limit, even if the chunk ends at\n",
    "    e.g. 476 tokens (within the ±tolerance band).\n",
    "\n",
    "    Returns a list of segment dicts, each inheriting the original path.\n",
    "    \"\"\"\n",
    "    content = segment[\"content\"]\n",
    "    total_tokens = token_count(content)\n",
    "\n",
    "    # If the segment already fits, return it as-is\n",
    "    if total_tokens <= limit:\n",
    "        return [segment]\n",
    "\n",
    "    # Split on sentence boundaries (full stop, exclamation mark, question mark)\n",
    "    sentences = re.split(r\"(?<=[.!?])\\s+\", content)\n",
    "\n",
    "    chunks = []\n",
    "    current_sentences = []\n",
    "    current_tokens = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence_tokens = token_count(sentence)\n",
    "\n",
    "        # If a single sentence exceeds the limit, keep it whole rather than\n",
    "        # cutting mid-sentence — the tolerance band permits this.\n",
    "        if current_tokens + sentence_tokens > limit + tolerance and current_sentences:\n",
    "            chunks.append({\n",
    "                \"path\": segment[\"path\"],\n",
    "                \"type\": segment[\"type\"],\n",
    "                \"content\": \" \".join(current_sentences),\n",
    "            })\n",
    "            current_sentences = []\n",
    "            current_tokens = 0\n",
    "\n",
    "        current_sentences.append(sentence)\n",
    "        current_tokens += sentence_tokens\n",
    "\n",
    "    # Flush the remaining sentences\n",
    "    if current_sentences:\n",
    "        chunks.append({\n",
    "            \"path\": segment[\"path\"],\n",
    "            \"type\": segment[\"type\"],\n",
    "            \"content\": \" \".join(current_sentences),\n",
    "        })\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# Apply chunking to all segments\n",
    "chunks = []\n",
    "for seg in all_segments:\n",
    "    chunks.extend(chunk_segment(seg))\n",
    "\n",
    "print(f\"Total chunks after splitting: {len(chunks):,}\")\n",
    "\n",
    "# Show token distribution\n",
    "token_counts = [token_count(c[\"content\"]) for c in chunks]\n",
    "print(f\"Token range: {min(token_counts)} – {max(token_counts)}\")\n",
    "print(f\"Mean tokens per chunk: {sum(token_counts) / len(token_counts):.0f}\")\n",
    "\n",
    "# Show how many exceed the limit (should only be single long sentences)\n",
    "over_limit = sum(1 for t in token_counts if t > CHUNK_TOKEN_LIMIT)\n",
    "print(f\"Chunks exceeding {CHUNK_TOKEN_LIMIT} tokens: {over_limit}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "j8wspc4hntl",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "026a590cd6ee48a49c7a69d832c1b652",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 'all-MiniLM-L6-v2' on cuda\n",
      "Embedding 232 chunks...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6479073930c84186b91a5abddea6a9c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shape: (232, 384)\n",
      "\n",
      "Stored 232 chunks in collection 'nvda_10_q'\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Cell 5 — Embed & Store in ChromaDB\n",
    "# =============================================================================\n",
    "\n",
    "# Load the embedding model on GPU\n",
    "model = SentenceTransformer(EMBEDDING_MODEL_NAME, device=DEVICE)\n",
    "print(f\"Loaded '{EMBEDDING_MODEL_NAME}' on {DEVICE}\")\n",
    "\n",
    "# Prepare texts and metadata\n",
    "texts = [c[\"content\"] for c in chunks]\n",
    "metadatas = [\n",
    "    {\n",
    "        \"path\": c[\"path\"],\n",
    "        \"type\": c[\"type\"],\n",
    "        \"ticker\": TICKER,\n",
    "        \"form_type\": FORM_TYPE,\n",
    "    }\n",
    "    for c in chunks\n",
    "]\n",
    "ids = [f\"{TICKER}_{FORM_TYPE}_{i}\" for i in range(len(chunks))]\n",
    "\n",
    "# Generate embeddings (batch encoding on GPU)\n",
    "print(f\"Embedding {len(texts):,} chunks...\")\n",
    "embeddings = model.encode(texts, show_progress_bar=True, convert_to_numpy=True)\n",
    "print(f\"Embedding shape: {embeddings.shape}\")\n",
    "\n",
    "# Initialise ChromaDB (persistent local storage)\n",
    "chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "collection_name = f\"{TICKER}_{FORM_TYPE}\".lower().replace(\"-\", \"_\")\n",
    "\n",
    "# Delete existing collection if present (for clean re-runs)\n",
    "try:\n",
    "    chroma_client.delete_collection(name=collection_name)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "collection = chroma_client.create_collection(\n",
    "    name=collection_name,\n",
    "    metadata={\"hnsw:space\": \"cosine\"},\n",
    ")\n",
    "\n",
    "# Upsert into ChromaDB\n",
    "collection.add(\n",
    "    ids=ids,\n",
    "    embeddings=embeddings.tolist(),\n",
    "    documents=texts,\n",
    "    metadatas=metadatas,\n",
    ")\n",
    "print(f\"\\nStored {collection.count():,} chunks in collection '{collection_name}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fuo82bjm7fm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Cell 6 — Semantic Search Function\n",
    "# =============================================================================\n",
    "\n",
    "def semantic_search(query, top_k=TOP_K):\n",
    "    \"\"\"Embed the query and return the top-k most relevant chunks.\"\"\"\n",
    "    query_embedding = model.encode([query], convert_to_numpy=True).tolist()\n",
    "\n",
    "    results = collection.query(\n",
    "        query_embeddings=query_embedding,\n",
    "        n_results=top_k,\n",
    "        include=[\"documents\", \"metadatas\", \"distances\"],\n",
    "    )\n",
    "\n",
    "    print(f\"Query: \\\"{query}\\\"\\n\")\n",
    "    print(f\"{'Rank':<5} {'Score':<8} {'Section Path'}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    for i in range(len(results[\"ids\"][0])):\n",
    "        # ChromaDB returns cosine distance; similarity = 1 - distance\n",
    "        distance = results[\"distances\"][0][i]\n",
    "        similarity = 1 - distance\n",
    "        path = results[\"metadatas\"][0][i][\"path\"]\n",
    "        doc = results[\"documents\"][0][i]\n",
    "        seg_type = results[\"metadatas\"][0][i][\"type\"]\n",
    "\n",
    "        print(f\"\\n#{i + 1:<4} {similarity:.4f}  [{seg_type}] {path}\")\n",
    "        print(f\"     {doc[:200]}...\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "umovo9i4km",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: \"What are the main risk factors?\"\n",
      "\n",
      "Rank  Score    Section Path\n",
      "================================================================================\n",
      "\n",
      "#1    0.4902  [text] NOTES TO CONDENSED CONSOLIDATED FINANCIAL STATEMENTS > ITEM 1A. RISK FACTORS > We may not be able to realize the potential benefits of business investments or acquisitions, and we may not be able to successfully integrate acquisition targets, which could hurt our ability to grow our business, develop new products or sell our products.\n",
      "     Additional risks related to acquisitions include, but are not limited to:...\n",
      "\n",
      "#2    0.4467  [text] NOTES TO CONDENSED CONSOLIDATED FINANCIAL STATEMENTS > ITEM 1A. RISK FACTORS > We may not be able to realize the potential benefits of business investments or acquisitions, and we may not be able to successfully integrate acquisition targets, which could hurt our ability to grow our business, develop new products or sell our products.\n",
      "     •exposure to additional cybersecurity risks and vulnerabilities; and...\n",
      "\n",
      "#3    0.4301  [text] NOTES TO CONDENSED CONSOLIDATED FINANCIAL STATEMENTS > ITEM 1A. RISK FACTORS\n",
      "     Other than the risk factors listed below, there have been no material changes from the risk factors previously described under Item 1A of our Annual Report on Form 10-K for the fiscal year ended Janua...\n",
      "\n",
      "#4    0.3880  [text] NOTES TO CONDENSED CONSOLIDATED FINANCIAL STATEMENTS > ITEM 1A. RISK FACTORS\n",
      "     Purchasing or owning NVIDIA common stock involves investment risks including, but not limited to, the risks described in Item 1A of our Annual Report on Form 10-K for the fiscal year ended January 29,...\n",
      "\n",
      "#5    0.3762  [text] NOTES TO CONDENSED CONSOLIDATED FINANCIAL STATEMENTS > ITEM 2. MANAGEMENT’S DISCUSSION AND ANALYSIS OF FINANCIAL CONDITION AND RESULTS OF OPERATIONS > Demand and Supply, Product Transitions, and New Products and Business Models\n",
      "     We build finished products and maintain inventory in advance of anticipated demand. While we have entered into long-term supply and capacity commitments, we may not be able to secure sufficient commit...\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Query: \"Revenue and net income figures\"\n",
      "\n",
      "Rank  Score    Section Path\n",
      "================================================================================\n",
      "\n",
      "#1    0.6266  [table] NOTES TO CONDENSED CONSOLIDATED FINANCIAL STATEMENTS > NOTES TO CONDENSED CONSOLIDATED FINANCIAL STATEMENTS (Continued)\n",
      "     | Compute & Networking | Graphics | All Other | Consolidated\n",
      " | (In millions) | (In millions) | (In millions) | (In millions)\n",
      "Three Months Ended October 29, 2023 |  |  |  | \n",
      "Revenue | $14,645 | $3,475...\n",
      "\n",
      "#2    0.5974  [text] NOTES TO CONDENSED CONSOLIDATED FINANCIAL STATEMENTS > NOTES TO CONDENSED CONSOLIDATED FINANCIAL STATEMENTS (Continued)\n",
      "     for cost of revenue and operating expenses, respectively, which resulted in an increase in operating income of $107 million and net income of $91 million after tax, or $0.04 per both basic and diluted...\n",
      "\n",
      "#3    0.5736  [table] NOTES TO CONDENSED CONSOLIDATED FINANCIAL STATEMENTS > ITEM 2. MANAGEMENT’S DISCUSSION AND ANALYSIS OF FINANCIAL CONDITION AND RESULTS OF OPERATIONS > Third Quarter of Fiscal Year 2024 Summary > Results of Operations\n",
      "     Results of Operations\n",
      "The following table sets forth, for the periods indicated, certain items in our Condensed Consolidated Statements of Income expressed as a percentage of revenue.\n",
      " | Three Months ...\n",
      "\n",
      "#4    0.5609  [text] NOTES TO CONDENSED CONSOLIDATED FINANCIAL STATEMENTS > ITEM 2. MANAGEMENT’S DISCUSSION AND ANALYSIS OF FINANCIAL CONDITION AND RESULTS OF OPERATIONS > Third Quarter of Fiscal Year 2024 Summary > Revenue\n",
      "     Revenue for the third quarter and first nine months of fiscal year 2024 was $18.12 billion and $38.82 billion, up 206% and 86%, respectively....\n",
      "\n",
      "#5    0.5473  [table] NOTES TO CONDENSED CONSOLIDATED FINANCIAL STATEMENTS > ITEM 2. MANAGEMENT’S DISCUSSION AND ANALYSIS OF FINANCIAL CONDITION AND RESULTS OF OPERATIONS > Other Income (Expense), Net\n",
      "     Other Income (Expense), Net\n",
      " | Three Months Ended\n",
      "October 29,\n",
      "2023 | Three Months Ended\n",
      "October 30,\n",
      "2022 | Three Months Ended\n",
      "$\n",
      "Change | Nine Months Ended\n",
      "October 29,\n",
      "2023 | Nine Months Ended\n",
      "October ...\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Query: \"Supply chain and manufacturing operations\"\n",
      "\n",
      "Rank  Score    Section Path\n",
      "================================================================================\n",
      "\n",
      "#1    0.6180  [text] NOTES TO CONDENSED CONSOLIDATED FINANCIAL STATEMENTS > ITEM 2. MANAGEMENT’S DISCUSSION AND ANALYSIS OF FINANCIAL CONDITION AND RESULTS OF OPERATIONS > Demand and Supply, Product Transitions, and New Products and Business Models > Global Trade\n",
      "     While we work to enhance the resiliency and redundancy of our supply chain, which is currently concentrated in the Asia-Pacific, including China, Hong Kong, Korea and Taiwan, new and existing export c...\n",
      "\n",
      "#2    0.5982  [text] NOTES TO CONDENSED CONSOLIDATED FINANCIAL STATEMENTS > ITEM 2. MANAGEMENT’S DISCUSSION AND ANALYSIS OF FINANCIAL CONDITION AND RESULTS OF OPERATIONS > Demand and Supply, Product Transitions, and New Products and Business Models > Macroeconomic Factors\n",
      "     Macroeconomic factors, including inflation, increased interest rates, capital market volatility, global supply chain constraints and global economic and geopolitical developments, may have direct and ...\n",
      "\n",
      "#3    0.5663  [text] NOTES TO CONDENSED CONSOLIDATED FINANCIAL STATEMENTS > ITEM 2. MANAGEMENT’S DISCUSSION AND ANALYSIS OF FINANCIAL CONDITION AND RESULTS OF OPERATIONS > Demand and Supply, Product Transitions, and New Products and Business Models\n",
      "     We build finished products and maintain inventory in advance of anticipated demand. While we have entered into long-term supply and capacity commitments, we may not be able to secure sufficient commit...\n",
      "\n",
      "#4    0.5191  [text] NOTES TO CONDENSED CONSOLIDATED FINANCIAL STATEMENTS > ITEM 1A. RISK FACTORS > Failure to estimate customer demand properly has led and could lead to mismatches between supply and demand.\n",
      "     If we underestimate our customers' future demand for our products, our foundry partners may not have adequate lead-time or capacity to increase production and we may not be able to obtain sufficient i...\n",
      "\n",
      "#5    0.5089  [text] NOTES TO CONDENSED CONSOLIDATED FINANCIAL STATEMENTS > NOTES TO CONDENSED CONSOLIDATED FINANCIAL STATEMENTS (Continued) > Note 13 - Commitments and Contingencies > Purchase Obligations\n",
      "     As of October 29, 2023, we had outstanding inventory purchase and long-term supply and capacity obligations totaling $17.11 billion. We enter into agreements with contract manufacturers that allow the...\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Cell 7 — Demo Queries\n",
    "# =============================================================================\n",
    "\n",
    "# Example queries — adjust to match the filing content\n",
    "_ = semantic_search(\"What are the main risk factors?\")\n",
    "print(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
    "_ = semantic_search(\"Revenue and net income figures\")\n",
    "print(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
    "_ = semantic_search(\"Supply chain and manufacturing operations\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uv_SEC_SemanticSearch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
